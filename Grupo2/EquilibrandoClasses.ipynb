# Equilibrando Classes

 O objetivo desse projeto é analisar o impacto das técnicas de reamostragem em datasets com a variável target desbalanceada, verificando as métricas de modelos de Machine Learning treinados com e sem aplicação das técnicas de reamostragem.

O dataset escolhido é https://www.kaggle.com/datasets/blastchar/telco-customer-churn, sobre a evasão de clientes na área de telecom.

# Etapa 1 - Análise Exploratória

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

### Importando o dataset

import kagglehub

# Download latest version
path = kagglehub.dataset_download("blastchar/telco-customer-churn")

print("Path to dataset files:", path)

df = pd.read_csv(r"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv")
#Cole o caminho ate o arquivo

df.head()


df.info()

df.shape

A variável TotalCharges é do tipo object, precisamos convertê-la para um float.

df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')

df.duplicated().sum()

df.info()

df.isnull().sum()

Verificamos a presença de 11 valores nulos para TotalCharges. Então, vamos remover essas linhas do dataset.

df = df.dropna()

df[['Churn', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']] = df[['Churn', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']].replace({'Yes': 1, 'No': 0})
df = df.drop('customerID', axis=1)

Vamos converter as variáveis booleanas (no dataset estão como sim e não) para 1 e 0. Vamos remover também a coluna de ID do cliente, já que ela não é pertinente na análise exploratória nem no treinamento dos modelos de ML.

df.describe()

df['Churn'].mean()

Isso indica que 26,58% dos consumidores evadiram os serviços da empresa, indicando um desbalanceamento so dataset em relação à variável alvo.

##Análise da variável alvo

df_groupby = df.groupby('Churn')
df_groupby.mean(numeric_only=True)

A média das variáveis agrupadas pela evasão indica que entre os clientes que evadiram:
*   Há mais idosos
*   Há mais solteiros
*   Há menos clientes com dependentes
*   Assinaram o serviço por menos tempo
*   Utilizaram mais pagamento digital
*   Pagaram mais caro nas contas mensais


df_groupby.size()

plt.pie(df_groupby.size(), labels=['Falso', 'Verdadeiro'], autopct='%1.1f%%')
plt.title("Distribuição relativa por evasão")
plt.show()

## Análise das variáveis categóricas

### Parceiro

df_partner = df.groupby('Partner')
plt.pie(df_partner.size(), labels=['Falso', 'Verdadeiro'], autopct='%1.1f%%')
plt.title("Distribuição relativa por parceiro")
plt.show()

sns.barplot(data=df, x='Partner', y='Churn', estimator='mean')
plt.title('Taxa de evasão por parceiro')
plt.xticks([0, 1], ['Não', 'Sim'])
plt.xlabel('Parceiro')
plt.ylabel('Taxa de evasão')
plt.show()

A taxa de evasão dos clientes solteiros é maior do que entre os clientes casados.

### Idade

df_senior = df.groupby('SeniorCitizen')
plt.pie(df_senior.size(), labels=['Não idoso', 'Idoso'], autopct='%1.1f%%')
plt.title("Distribuição relativa por idade")
plt.show()

sns.barplot(data=df, x='SeniorCitizen', y='Churn', estimator='mean')
plt.title('Taxa de evasão por Idade')
plt.xticks([0, 1], ['Não idoso', 'Idoso'])
plt.ylabel('Taxa de evasão')
plt.show()

Os idosos são a minoria entre os clientes e possuem taxa de evasão maior em comparação à dos mais jovens.

### Tipo de contrato

df_contract = df.groupby('Contract')
df_contract.size()

plt.pie(df_contract.size(), labels=['Mês a mês', 'Um ano', 'Dois anos'], autopct='%1.1f%%')
plt.title("Distribuição relativa por parceiro")
plt.show()

sns.barplot(data=df, x='Contract', y='Churn', estimator='mean')
plt.title('Taxa de evasão por contrato')
plt.xticks(['Month-to-month', 'One year', 'Two year'], ['Mês a mês', 'Um ano', 'Dois anos'])
plt.ylabel('Taxa de evasão')
plt.xlabel('Contrato')
plt.show()

Nota-se que os contratos mês a mês são maioria e a taxa de evasão nesses é significativamente maior em relação as demais modalidades de contrato.

### Dependentes

df_dependents = df.groupby('Dependents')
df_dependents.size()

plt.pie(df_dependents.size(), labels=['Não', 'Sim'], autopct='%1.1f%%')
plt.title("Distribuição relativa por dependentes")
plt.show()

sns.barplot(data=df, x='Dependents', y='Churn', estimator='mean')
plt.title('Taxa de evasão por presença de dependentes')
plt.xticks([0, 1], ['Não', 'Sim'])
plt.ylabel('Taxa de evasão')
plt.xlabel('Dependentes')
plt.show()

### Gênero

df_gender = df.groupby('gender')
df_gender.size()

plt.pie(df_gender.size(), labels=['Feminino', 'Masculino'], autopct='%1.1f%%')
plt.title("Distribuição relativa por sexo")
plt.show()

sns.barplot(data=df, x='gender', y='Churn', estimator='mean')
plt.title('Taxa de evasão por gênero')
plt.xticks([0, 1], ['Feminino', 'Masculino'])
plt.xlabel('Gênero')
plt.ylabel('Taxa de evasão')
plt.show()

### Pagamento digital

df_paperless = df.groupby('PaperlessBilling')
df_paperless.size()

plt.pie(df_paperless.size(), labels=['Sim', 'Não'], autopct='%1.1f%%')
plt.title("Distribuição relativa por pagamento digital")
plt.show()

sns.barplot(data=df, x='PaperlessBilling', y='Churn', estimator='mean')
plt.title('Taxa de evasão por pagamento digital')
plt.xticks([0, 1], ['Não', 'Sim'])
plt.ylabel('Taxa de evasão')
plt.xlabel('Pagamento digital')
plt.show()

### Distribuição das demais variáveis

services = ['PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']
plt.figure(figsize=(20, 15))
for i, service in enumerate(services):
    plt.subplot(3, 3, i+1)
    sns.countplot(data=df, x=service, hue=service, palette='viridis', legend=False)
    plt.ylabel('Quantidade')
    plt.title(service)
plt.tight_layout()
plt.show()

plt.figure(figsize=(20, 15))
for i, service in enumerate(services):
    plt.subplot(3, 3, i+1)
    sns.barplot(data=df, x=service, hue=service, palette='viridis', legend=False, y='Churn')
    plt.ylabel('Taxa de evasão')
    plt.title(service)
plt.tight_layout()
plt.show()

## Análise das variáveis numéricas

### Custo mensal

sns.histplot(data=df, x='MonthlyCharges', kde=True, bins= 20)
plt.ylabel('Quantidade')
plt.xlabel('Custo mensal')
plt.title("Distribuição por custo mensal")
plt.show()

Por esse gráfico, observa-se que boa parte dos clientes dessa empresa tem contratos mais em conta, pagam menos por mês. Porém, a maior parte dos clientes paga entre 40 e 120 dólares.

plt.figure(figsize=(12, 5))

plt.subplot(1,2,2)
sns.boxplot(data=df, y='MonthlyCharges', hue='Churn')
plt.ylabel('Quantidade')
plt.title("Boxplot por custo mensal")

plt.subplot(1,2,1)
sns.histplot(data=df, x='MonthlyCharges', hue='Churn', bins= 20)
plt.ylabel('Quantidade')
plt.xlabel('Custo mensal')
plt.title("Distribuição por custo mensal")

### Tempo de assinatura do serviço

sns.histplot(data=df, x='tenure', kde=True, bins= 20)
plt.ylabel('Quantidade')
plt.xlabel('Tempo de assinatura')
plt.title("Distribuição por tempo de assinatura")
plt.show()

plt.figure(figsize=(12, 5))

plt.subplot(1,2,2)
sns.boxplot(data=df, y='tenure', hue='Churn' )
plt.ylabel('Quantidade')
plt.title("Boxplot por tempo de assinatura")

plt.subplot(1,2,1)
sns.histplot(data=df, x='tenure', hue='Churn', palette='viridis', bins= 20)
plt.ylabel('Quantidade')
plt.xlabel('Tempo de assinatura')
plt.title("Distribuição por tempo de assinatura")

Como pode-se ver pelo primeiro gráfico, as pessoas que tendem a fechar contrato (Churn = 1) não passam muito tempo com a empresa.

Pelo segundo gráfico (boxplot) é possível observar alguns outliers, que fecharam contrato mesmo com muito tempo com a empresa.

plt.figure(figsize=(15, 5), frameon=False)
plt.suptitle("Distribuição por tempo de assinatura e contrato\n")

plt.subplot(1, 3, 1)
TwoYear = df[df['Contract'] == 'Two year']
sns.histplot(data=TwoYear, x='tenure', kde=True, bins= 20, color='orange')
plt.ylabel('Quantidade')
plt.ylim(0,1200)
plt.xlabel('Tempo de assinatura')
plt.title("Contrato de 2 anos")


plt.subplot(1, 3, 2)
OneYear = df[df['Contract'] == 'One year']
sns.histplot(data=OneYear, x='tenure', kde=True, bins= 20, color='purple')
plt.ylabel('Quantidade')
plt.ylim(0,1200)
plt.xlabel('Tempo de assinatura')
plt.title("Contrato de 1 ano")

plt.subplot(1, 3, 3)
Mtm = df[df['Contract'] == 'Month-to-month']
sns.histplot(data=Mtm, x='tenure', kde=True, bins= 20, color='black')
plt.ylabel('Quantidade')
plt.ylim(0,1200)
plt.xlabel('Tempo de assinatura')
plt.title("Contrato mês a mês")

plt.tight_layout()
plt.show()

Os três gráficos acima mostram as tendências referentes a cada tipo de contrato:
* Boa parte dos clientes da empresa tem contratos de 2 anos e esses são os que permanecem por mais tempo.
* Poucos clientes optam pelo contrato de 1 ano
* A maioria dos clientes dessa empresa tem contratos mês a mês, e esses são os que passam menos tempo na empresa, com grande quantidade de Churn = 1.

fig, ax = plt.subplots(figsize=(10, 10))
sns.scatterplot(x='MonthlyCharges', y='tenure', hue = 'Churn', data=df)
plt.ylabel('Tempo de assinatura')
plt.xlabel('Custo mensal')
plt.show()

O gráfico de dispersão acima mostra que, em média, pessoas que deixaram a empresa (Churn = 1) tinham um custo mensal maior e estavam na empresa por menos tempo que as que permaneceram (Churn = 0)

### Custo total pago

sns.histplot(data=df, x='TotalCharges', kde=True, bins= 20)
plt.title("Distribuição por custo total pago")
plt.xlabel('Custo total')
plt.ylabel('Quantidade')
plt.show()

O gráfico mostra que a maioria dos clientes atuais dessa empresa pagaram pouco no total, o que indica que estão com a empresa há pouco tempo.

plt.figure(figsize=(12, 5))

plt.subplot(1,2,2)
sns.boxplot(data=df, y='TotalCharges', hue='Churn' )
plt.ylabel('Quantidade')
plt.title("Boxplot por custo total pago")

plt.subplot(1,2,1)
sns.histplot(data=df, x='TotalCharges', hue='Churn', palette='viridis', bins= 20)
plt.ylabel('Quantidade')
plt.xlabel('Custo total')
plt.title("Distribuição por custo total pago")

No BoxPlot pode-se observar um número relevante de outliers, clientes que deixaram a empresa mesmo com um grande valor total pago com o tempo.

outliers_custo = df[df['TotalCharges'] > 6000]
sns.histplot(data=outliers_custo, x='TotalCharges', hue='Churn', bins= 20, palette='viridis')
plt.ylabel('Quantidade')
plt.xlabel('Custo total')
plt.title("Distribuição por custo total pago")

O gráfico acima mostra que a maioria das pessoas que continuaram com seus contratos já pagaram um total maior em comparação com os clientes que saíram. Isso corrobora com o gráfico anterior sobre tempo de contrato e o abandono dele.

df_heatmap = df.select_dtypes(include=['number'])
plt.figure(figsize=(8, 8))
sns.heatmap(df_heatmap.corr(), annot=True)
plt.show()

Como é possível observar pelo mapa de calor, há correlação entre as seguintes variáveis:



---



*   TotalCharges (Total pago) x Tenure (Tempo de contrato)
    * Faz sentido, pois o total pago durante o tempo com a empresa aumenta com relação ao tempo de contratos.
*   Churn (Se cancelou ou não) x Tenure
    * Assim como foi visto em gráficos plotados, clientes que passam mais tempo com a empresa tendem a manter lealdade.
*   Tenure x Partner
    * 0.38 -> Pessoas com parceiros tendem a manter contratos mais longos



# Etapa 1 - Machine Learning

## Divisão Treino Validação e Teste

Faremos primeiramente a divisão dos dados em treino, validação e teste antes de padronizá-los a fim de evitar vazamento de dados

df_dummies = pd.get_dummies(df) # One-hot Encoding
df_dummies = df_dummies.replace({True: 1, False: 0})
df_dummies


X = df_dummies.drop('Churn', axis=1)
y = df_dummies['Churn'].values

from sklearn.model_selection import train_test_split
X_treino_val, X_teste, y_treino_val, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)
X_treino, X_val, y_treino, y_val = train_test_split(X_treino_val, y_treino_val, test_size=0.25, random_state=42)

print("Proporção Churn - Treino: ", y_treino.mean())
print("Proporção Churn - Validação: ", y_val.mean())
print("Proporção Churn - Teste: ", y_teste.mean())

## Padronização dos dados

### Escalando as variáveis para valores entre 0 e 1

from sklearn.preprocessing import MinMaxScaler
variaveis = X.columns.values
scaler = MinMaxScaler(feature_range=(0, 1))
scaler.fit(X_treino)
X_treino = pd.DataFrame(scaler.transform(X_treino))
X_val = pd.DataFrame(scaler.transform(X_val))
X_teste = pd.DataFrame(scaler.transform(X_teste))

## KNN

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score,f1_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

knn = KNeighborsClassifier()

### Treino

knn.fit(X_treino, y_treino)

### Validação

y_val_proba_knn = knn.predict_proba(X_val)[:, 1]
y_val_pred_knn = knn.predict(X_val)

acuracia_val_knn = accuracy_score(y_val, y_val_pred_knn)
precisao_val_knn = precision_score(y_val, y_val_pred_knn)
recall_val_knn = recall_score(y_val, y_val_pred_knn)
f1_val_knn = f1_score(y_val, y_val_pred_knn)
roc_auc_val_knn = roc_auc_score(y_val, y_val_proba_knn)

print("Acurácia na validação:", acuracia_val_knn)
print("Precisão na validação:", precisao_val_knn)
print("Recall na validação:", recall_val_knn)
print("f1-Score na validação:", f1_val_knn)
print("ROC AUC na validação:", roc_auc_val_knn)

cm_val_knn = confusion_matrix(y_val, y_val_pred_knn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_knn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Tuning dos hiperparâmetros

n_neighbors = list(range(5, 83, 2)) # números impares de 5 ate 83
weights = ['uniform', 'distance']
algorithm= ['auto']
leaf_size= [30]
p= [2, 1]
metric=['minkowski', 'manhattan']
n_jobs= [-1]

parametros = {
              'n_neighbors': n_neighbors,
               'weights': weights,
               'algorithm': algorithm,
               'leaf_size': leaf_size,
               'p': p,
               'metric': metric,
               'n_jobs': n_jobs
        }

grid_search_knn = GridSearchCV(
                                knn,
                                parametros,
                                cv=5, # validação cruzada com 5 folds
                                scoring='recall' # otimizando para recall
                                )
grid_search_knn.fit(X_treino, y_treino)

Otimizaremos para recall, pois essa métrica indica a proporção de positivos reais que foram corretamente classificados como positivos. Com isso, minimizamos a taxa de falsos negativos, que possuem maior custo na predição, já que indicam a classificação de um cliente que evadiria como um que não evadiria, trazendo malefícios para o propósito de negócios do modelo preditivo.

print("Melhores hiperparâmetros:", grid_search_knn.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_knn.best_score_)

### Teste

y_teste_proba_knn = grid_search_knn.predict_proba(X_teste)[:, 1]
y_teste_pred_knn = grid_search_knn.predict(X_teste)

acuracia_teste_knn = accuracy_score(y_teste, y_teste_pred_knn)
precisao_teste_knn = precision_score(y_teste, y_teste_pred_knn)
recall_teste_knn = recall_score(y_teste, y_teste_pred_knn)
f1_teste_knn = f1_score(y_teste, y_teste_pred_knn)
roc_auc_teste_knn = roc_auc_score(y_teste, y_teste_proba_knn)

print("Acurácia no teste:", acuracia_teste_knn)
print("Precisão no teste:", precisao_teste_knn)
print("Recall no teste:", recall_teste_knn)
print("f1-Score no teste:", f1_teste_knn)
print("ROC AUC no teste:", roc_auc_teste_knn)

cm_teste_knn = confusion_matrix(y_teste, y_teste_pred_knn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_knn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

## Random Forest

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42) # reproductiblidade

### Treino

rf.fit(X_treino, y_treino)

### Validação

y_val_proba_rf = rf.predict_proba(X_val)[:, 1]
y_val_pred_rf = rf.predict(X_val)

acuracia_val_rf = accuracy_score(y_val, y_val_pred_rf)
precisao_val_rf = precision_score(y_val, y_val_pred_rf)
recall_val_rf = recall_score(y_val, y_val_pred_rf)
f1_val_rf = f1_score(y_val, y_val_pred_rf)
roc_auc_val_rf = roc_auc_score(y_val, y_val_proba_rf)

print("Acurácia na validação:", acuracia_val_rf)
print("Precisão na validação:", precisao_val_rf)
print("Recall na validação:", recall_val_rf)
print("f1-Score na validação:", f1_val_rf)
print("ROC AUC na validação:", roc_auc_val_rf)

cm_val_rf = confusion_matrix(y_val, y_val_pred_rf)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_rf, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Tuning dos hiperparâmetros

# Numero de arvores em floresta aleatoria
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)]
# Numero de features a serem consideradas em cada splita
max_features = ['log2', 'sqrt']
# Profundidade maxima da arvore
max_depth = [1, 6, 11, 16, 21, None]
# Numero minimo de amostras para separar um nó
min_samples_split = [2, 5, 10]
# Numero minimo de amostras para cada nó de folha
min_samples_leaf = [1, 2, 4]

parametros = {
               'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              }

random_search_rf = RandomizedSearchCV(rf, parametros, cv=5, scoring='recall', n_jobs=-1, random_state=42)
random_search_rf.fit(X_treino, y_treino)

print("Melhores hiperparâmetros:", random_search_rf.best_params_)
print("Melhor Recall na validação cruzada:", random_search_rf.best_score_)

### Teste

y_teste_proba_rf = random_search_rf.predict_proba(X_teste)[:, 1]
y_teste_pred_rf = random_search_rf.predict(X_teste)

acuracia_teste_rf = accuracy_score(y_teste, y_teste_pred_rf)
precisao_teste_rf = precision_score(y_teste, y_teste_pred_rf)
recall_teste_rf = recall_score(y_teste, y_teste_pred_rf)
f1_teste_rf = f1_score(y_teste, y_teste_pred_rf)
roc_auc_teste_rf = roc_auc_score(y_teste, y_teste_proba_rf)

print("Acurácia no teste:", acuracia_teste_rf)
print("Precisão no teste:", precisao_teste_rf)
print("Recall no teste:", recall_teste_rf)
print("f1-Score no teste:", f1_teste_rf)
print("ROC AUC no teste:", roc_auc_teste_rf)

cm_teste_rf = confusion_matrix(y_teste, y_teste_pred_rf)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_rf, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

## Regressão Logística

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(
                        random_state=42, # reproductibilidade
                        solver='liblinear', # bom para datasets pequenos
                        max_iter=1000 # permite alto número de iterações até encontrar a convergência
                        )

### Treino

lr.fit(X_treino, y_treino)

print('Convergência do modelo:', lr.n_iter_)

coeficientes = pd.DataFrame({'Variável': X.columns, 'Coeficiente': lr.coef_[0]})
coeficientes = coeficientes.sort_values(by='Coeficiente', ascending=False)
print(coeficientes)

Esses coeficientes representam o impacto das variáveis na probabilidade de evasão. Um coeficiente positivo indica que quando esse valor cresce, também cresce a probabilidade de evasão, e vice-versa para os coeficientes negativos.

Maiores coeficientes beta:
  * Custo total
  * Contrato mês a mês
  * Serviço de internet fibra óptica

Menores coeficientes beta:
  * Tempo de assinatura do serviço
  * Contrato de dois anos
  * Serviço de internet DSL

Reforçando o que foi observado na análise exploratória dos dados

###Validação

y_val_proba_lr = lr.predict_proba(X_val)[:, 1]
y_val_pred_lr = lr.predict(X_val)

acuracia_val_lr = accuracy_score(y_val, y_val_pred_lr)
precisao_val_lr = precision_score(y_val, y_val_pred_lr)
recall_val_lr = recall_score(y_val, y_val_pred_lr)
f1_val_lr = f1_score(y_val, y_val_pred_lr)
roc_auc_val_lr = roc_auc_score(y_val, y_val_proba_lr)

print("Acurácia na validação:", acuracia_val_lr)
print("Precisão na validação:", precisao_val_lr)
print("Recall na validação:", recall_val_lr)
print("f1-Score na validação:", f1_val_lr)
print("ROC AUC na validação:", roc_auc_val_lr)

cm_val_lr = confusion_matrix(y_val, y_val_pred_lr)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_lr, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Tuning dos hiperparâmetros

parametros = {
              'C': [0.001, 0.01, 0.1, 1, 10, 100], # inverso da força de regularização
              'penalty': ['l1', 'l2'], # penalidade aplicada
              'class_weight': ['balanced', None], # pesos associados às classes, 'balanced' ajusta os pesos de modo inversamente proporcional a frequência das classes
        }

grid_search_lr = GridSearchCV(
                                lr,
                                parametros,
                                cv=5, # validação cruzada com 5 fols
                                scoring='recall' # otimizando para recall
                              )
grid_search_lr.fit(X_treino, y_treino)

print("Melhores hiperparâmetros:", grid_search_lr.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_lr.best_score_)

### Teste

y_teste_proba_lr = grid_search_lr.predict_proba(X_teste)[:, 1]
y_teste_pred_lr = grid_search_lr.predict(X_teste)

acuracia_teste_lr = accuracy_score(y_teste, y_teste_pred_lr)
precisao_teste_lr = precision_score(y_teste, y_teste_pred_lr)
recall_teste_lr = recall_score(y_teste, y_teste_pred_lr)
f1_teste_lr = f1_score(y_teste, y_teste_pred_lr)
roc_auc_teste_lr = roc_auc_score(y_teste, y_teste_proba_lr)

print("Acurácia no teste:", acuracia_teste_lr)
print("Precisão no teste:", precisao_teste_lr)
print("Recall no teste:", recall_teste_lr)
print("f1-Score no teste:", f1_teste_lr)
print("ROC AUC no teste:", roc_auc_teste_lr)

Dentre os modelos testados, apresentou maior recall, métrica para qual otimizaremos primariamente a fim de evitar falsos negativos.

cm_teste_lr = confusion_matrix(y_teste, y_teste_pred_lr)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_lr, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

# Etapa 2 - Técnicas de reamostragem

## Random Oversampling

Esta técnica usa de um algoritmo que escolher randomicamente amostras da classe minoritária, com reposição, e as duplica a fim de equilibrar as classes.

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_treino_ros, y_treino_ros = ros.fit_resample(X_treino, y_treino)

print(X_treino.shape)
print(X_treino_ros.shape)

print("Proporção Churn - Treino: ", y_treino_ros.mean())
print("Proporção Churn - Validação: ", y_val.mean())
print("Proporção Churn - Teste: ", y_teste.mean())

### KNN - RandomOversampling


#### Treino

knn_ros = KNeighborsClassifier()

knn_ros.fit(X_treino_ros, y_treino_ros)

#### Validação

y_val_proba_knn_ros = knn_ros.predict_proba(X_val)[:, 1]
y_val_pred_knn_ros = knn_ros.predict(X_val)

acuracia_val_knn_ros = accuracy_score(y_val, y_val_pred_knn_ros)
precisao_val_knn_ros = precision_score(y_val, y_val_pred_knn_ros)
recall_val_knn_ros = recall_score(y_val, y_val_pred_knn_ros)
f1_val_knn_ros = f1_score(y_val, y_val_pred_knn_ros)
roc_auc_val_knn_ros = roc_auc_score(y_val, y_val_proba_knn_ros)

print("Acurácia na validação:", acuracia_val_knn_ros)
print("Precisão na validação:", precisao_val_knn_ros)
print("Recall na validação:", recall_val_knn_ros)
print("f1-Score na validação:", f1_val_knn_ros)
print("ROC AUC na validação:", roc_auc_val_knn_ros)

cm_val_knn_ros = confusion_matrix(y_val, y_val_pred_knn_ros)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_knn_ros, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

n_neighbors = list(range(5, 83, 2)) # números impares de 5 ate 83
weights = ['uniform', 'distance']
algorithm= ['auto']
leaf_size= [30]
p= [2, 1]
metric=['minkowski', 'manhattan']
n_jobs= [-1]

parametros = {
              'n_neighbors': n_neighbors,
               'weights': weights,
               'algorithm': algorithm,
               'leaf_size': leaf_size,
               'p': p,
               'metric': metric,
               'n_jobs': n_jobs
        }

grid_search_knn_ros = GridSearchCV(
                                knn_ros,
                                parametros,
                                cv=5, # validação cruzada com 5 folds
                                scoring='recall' # otimizando para recall
                                )
grid_search_knn_ros.fit(X_treino_ros, y_treino_ros)

print("Melhores hiperparâmetros:", grid_search_knn_ros.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_knn_ros.best_score_)

#### Teste

y_teste_ros_proba_knn = grid_search_knn_ros.predict_proba(X_teste)[:, 1]
y_teste_pred_knn_ros = grid_search_knn_ros.predict(X_teste)

acuracia_teste_knn_ros = accuracy_score(y_teste, y_teste_pred_knn_ros)
precisao_teste_knn_ros = precision_score(y_teste, y_teste_pred_knn_ros)
recall_teste_knn_ros = recall_score(y_teste, y_teste_pred_knn_ros)
f1_teste_knn_ros = f1_score(y_teste, y_teste_pred_knn_ros)
roc_auc_teste_knn_ros = roc_auc_score(y_teste, y_teste_proba_knn)

print("Acurácia no teste:", acuracia_teste_knn_ros)
print("Precisão no teste:", precisao_teste_knn_ros)
print("Recall no teste:", recall_teste_knn_ros)
print("f1-Score no teste:", f1_teste_knn_ros)
print("ROC AUC no teste:", roc_auc_teste_knn_ros)

cm_teste_knn_ros = confusion_matrix(y_teste, y_teste_pred_knn_ros)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_knn_ros, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Random Forest - RandomOversampling

rf_ros = RandomForestClassifier(random_state=42)

#### Treino

rf_ros.fit(X_treino_ros, y_treino_ros)

#### Validação

y_val_proba_rf_ros = rf_ros.predict_proba(X_val)[:, 1]
y_val_pred_rf_ros = rf_ros.predict(X_val)

acuracia_val_rf_ros = accuracy_score(y_val, y_val_pred_rf_ros)
precisao_val_rf_ros = precision_score(y_val, y_val_pred_rf_ros)
recall_val_rf_ros = recall_score(y_val, y_val_pred_rf_ros)
f1_val_rf_ros = f1_score(y_val, y_val_pred_rf_ros)
roc_auc_val_rf_ros = roc_auc_score(y_val, y_val_proba_rf_ros)

print("Acurácia na validação:", acuracia_val_rf_ros)
print("Precisão na validação:", precisao_val_rf_ros)
print("Recall na validação:", recall_val_rf_ros)
print("f1-Score na validação:", f1_val_rf_ros)
print("ROC AUC na validação:", roc_auc_val_rf_ros)

cm_val_rf_ros = confusion_matrix(y_val, y_val_pred_rf_ros)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_rf_ros, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

# Numero de arvores em floresta aleatoria
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)]
# Numero de features a serem consideradas em cada splita
max_features = ['log2', 'sqrt']
# Profundidade maxima da arvore
max_depth = [1, 6, 11, 16, 21, None]
# Numero minimo de amostras para separar um nó
min_samples_split = [2, 5, 10]
# Numero minimo de amostras para cada nó de folha
min_samples_leaf = [1, 2, 4]

parametros = {
               'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              }

random_search_rf_ros = RandomizedSearchCV(rf_ros, parametros, cv=5, scoring='recall', n_jobs=-1, random_state=42)
random_search_rf_ros.fit(X_treino_ros, y_treino_ros)

print("Melhores hiperparâmetros:", random_search_rf_ros.best_params_)
print("Melhor Recall na validação cruzada:", random_search_rf_ros.best_score_)

#### Teste

y_teste_proba_rf_ros = random_search_rf_ros.predict_proba(X_teste)[:, 1]
y_teste_pred_rf_ros = random_search_rf_ros.predict(X_teste)

acuracia_teste_rf_ros = accuracy_score(y_teste, y_teste_pred_rf_ros)
precisao_teste_rf_ros = precision_score(y_teste, y_teste_pred_rf_ros)
recall_teste_rf_ros = recall_score(y_teste, y_teste_pred_rf_ros)
f1_teste_rf_ros = f1_score(y_teste, y_teste_pred_rf_ros)
roc_auc_teste_rf_ros = roc_auc_score(y_teste, y_teste_proba_rf_ros)

print("Acurácia no teste:", acuracia_teste_rf_ros)
print("Precisão no teste:", precisao_teste_rf_ros)
print("Recall no teste:", recall_teste_rf_ros)
print("f1-Score no teste:", f1_teste_rf_ros)
print("ROC AUC no teste:", roc_auc_teste_rf_ros)

cm_teste_rf_ros = confusion_matrix(y_teste, y_teste_pred_rf_ros)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_rf_ros, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Regressão Logística - RandomOversampling

lr_ros = LogisticRegression(
                        random_state=42, # reproductibilidade
                        solver='liblinear', # bom para datasets pequenos
                        max_iter=1000 # permite alto número de iterações até encontrar a convergência
                        )

#### Treino

lr_ros.fit(X_treino_ros, y_treino_ros)

print('Convergência do modelo:', lr_ros.n_iter_)

coeficientes_ros = pd.DataFrame({'Variável': X.columns, 'Coeficiente': lr_ros.coef_[0]})
coeficientes_ros = coeficientes_ros.sort_values(by='Coeficiente', ascending=False)
print(coeficientes_ros)

#### Validação

y_val_proba_lr_ros = lr_ros.predict_proba(X_val)[:, 1]
y_val_pred_lr_ros = lr_ros.predict(X_val)

acuracia_val_lr_ros = accuracy_score(y_val, y_val_pred_lr_ros)
precisao_val_lr_ros = precision_score(y_val, y_val_pred_lr_ros)
recall_val_lr_ros = recall_score(y_val, y_val_pred_lr_ros)
f1_val_lr_ros = f1_score(y_val, y_val_pred_lr_ros)
roc_auc_val_lr_ros = roc_auc_score(y_val, y_val_proba_lr_ros)

print("Acurácia na validação:", acuracia_val_lr_ros)
print("Precisão na validação:", precisao_val_lr_ros)
print("Recall na validação:", recall_val_lr_ros)
print("f1-Score na validação:", f1_val_lr_ros)
print("ROC AUC na validação:", roc_auc_val_lr_ros)

cm_val_lr_ros = confusion_matrix(y_val, y_val_pred_lr_ros)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_lr_ros, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

parametros = {
              'C': [0.001, 0.01, 0.1, 1, 10, 100], # inverso da força de regularização
              'penalty': ['l1', 'l2'], # penalidade aplicada
              'class_weight': ['balanced', None], # pesos associados às classes, 'balanced' ajusta os pesos de modo inversamente proporcional a frequência das classes
        }

grid_search_lr_ros = GridSearchCV(
                                lr_ros,
                                parametros,
                                cv=5, # validação cruzada com 5 fols
                                scoring='recall' # otimizando para recall
                              )
grid_search_lr_ros.fit(X_treino_ros, y_treino_ros)

print("Melhores hiperparâmetros:", grid_search_lr_ros.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_lr_ros.best_score_)

#### Teste

y_teste_proba_lr_ros = grid_search_lr_ros.predict_proba(X_teste)[:, 1]
y_teste_pred_lr_ros = grid_search_lr_ros.predict(X_teste)

acuracia_teste_lr_ros = accuracy_score(y_teste, y_teste_pred_lr_ros)
precisao_teste_lr_ros = precision_score(y_teste, y_teste_pred_lr_ros)
recall_teste_lr_ros = recall_score(y_teste, y_teste_pred_lr_ros)
f1_teste_lr_ros = f1_score(y_teste, y_teste_pred_lr_ros)
roc_auc_teste_lr_ros = roc_auc_score(y_teste, y_teste_proba_lr_ros)

print("Acurácia no teste:", acuracia_teste_lr_ros)
print("Precisão no teste:", precisao_teste_lr_ros)
print("Recall no teste:", recall_teste_lr_ros)
print("f1-Score no teste:", f1_teste_lr_ros)
print("ROC AUC no teste:", roc_auc_teste_lr_ros)

cm_teste_lr_ros = confusion_matrix(y_teste, y_teste_pred_lr_ros)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_lr_ros, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

## Random Undersampling

Esta técnica de reamostragem exclui aleatoriamente amostras da classe majoritária a fim de equilibrar as classes.

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_treino_rus, y_treino_rus = rus.fit_resample(X_treino, y_treino)

print(X_treino.shape)
print(X_treino_rus.shape)

print("Proporção Churn - Treino: ", y_treino_rus.mean())
print("Proporção Churn - Validação: ", y_val.mean())
print("Proporção Churn - Teste: ", y_teste.mean())

### KNN - Random Undersampling


#### Treino

knn_rus = KNeighborsClassifier()

knn_rus.fit(X_treino_rus, y_treino_rus)

#### Validação

y_val_proba_knn_rus = knn_rus.predict_proba(X_val)[:, 1]
y_val_pred_knn_rus = knn_rus.predict(X_val)

acuracia_val_knn_rus = accuracy_score(y_val, y_val_pred_knn_rus)
precisao_val_knn_rus = precision_score(y_val, y_val_pred_knn_rus)
recall_val_knn_rus = recall_score(y_val, y_val_pred_knn_rus)
f1_val_knn_rus = f1_score(y_val, y_val_pred_knn_rus)
roc_auc_val_knn_rus = roc_auc_score(y_val, y_val_proba_knn_rus)

print("Acurácia na validação:", acuracia_val_knn_rus)
print("Precisão na validação:", precisao_val_knn_rus)
print("Recall na validação:", recall_val_knn_rus)
print("f1-Score na validação:", f1_val_knn_rus)
print("ROC AUC na validação:", roc_auc_val_knn_rus)

cm_val_knn_rus = confusion_matrix(y_val, y_val_pred_knn_rus)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_knn_rus, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

n_neighbors = list(range(5, 83, 2)) # números impares de 5 ate 83
weights = ['uniform', 'distance']
algorithm= ['auto']
leaf_size= [30]
p= [2, 1]
metric=['minkowski', 'manhattan']
n_jobs= [-1]

parametros = {
              'n_neighbors': n_neighbors,
               'weights': weights,
               'algorithm': algorithm,
               'leaf_size': leaf_size,
               'p': p,
               'metric': metric,
               'n_jobs': n_jobs
        }

grid_search_knn_rus = GridSearchCV(
                                knn_rus,
                                parametros,
                                cv=5, # validação cruzada com 5 folds
                                scoring='recall' # otimizando para recall
                                )
grid_search_knn_rus.fit(X_treino_rus, y_treino_rus)

print("Melhores hiperparâmetros:", grid_search_knn_rus.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_knn_rus.best_score_)

#### Teste

y_teste_rus_proba_knn = grid_search_knn_rus.predict_proba(X_teste)[:, 1]
y_teste_pred_knn_rus = grid_search_knn_rus.predict(X_teste)

acuracia_teste_knn_rus = accuracy_score(y_teste, y_teste_pred_knn_rus)
precisao_teste_knn_rus = precision_score(y_teste, y_teste_pred_knn_rus)
recall_teste_knn_rus = recall_score(y_teste, y_teste_pred_knn_rus)
f1_teste_knn_rus = f1_score(y_teste, y_teste_pred_knn_rus)
roc_auc_teste_knn_rus = roc_auc_score(y_teste, y_teste_proba_knn)

print("Acurácia no teste:", acuracia_teste_knn_rus)
print("Precisão no teste:", precisao_teste_knn_rus)
print("Recall no teste:", recall_teste_knn_rus)
print("f1-Score no teste:", f1_teste_knn_rus)
print("ROC AUC no teste:", roc_auc_teste_knn_rus)

cm_teste_knn_rus = confusion_matrix(y_teste, y_teste_pred_knn_rus)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_knn_rus, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Random Forest - Random Undersampling

rf_rus = RandomForestClassifier(random_state=42)

#### Treino

rf_rus.fit(X_treino_rus, y_treino_rus)

#### Validação

y_val_proba_rf_rus = rf_rus.predict_proba(X_val)[:, 1]
y_val_pred_rf_rus = rf_rus.predict(X_val)

acuracia_val_rf_rus = accuracy_score(y_val, y_val_pred_rf_rus)
precisao_val_rf_rus = precision_score(y_val, y_val_pred_rf_rus)
recall_val_rf_rus = recall_score(y_val, y_val_pred_rf_rus)
f1_val_rf_rus = f1_score(y_val, y_val_pred_rf_rus)
roc_auc_val_rf_rus = roc_auc_score(y_val, y_val_proba_rf_rus)

print("Acurácia na validação:", acuracia_val_rf_rus)
print("Precisão na validação:", precisao_val_rf_rus)
print("Recall na validação:", recall_val_rf_rus)
print("f1-Score na validação:", f1_val_rf_rus)
print("ROC AUC na validação:", roc_auc_val_rf_rus)

cm_val_rf_rus = confusion_matrix(y_val, y_val_pred_rf_rus)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_rf_rus, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

# Numero de arvores em floresta aleatoria
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)]
# Numero de features a serem consideradas em cada splita
max_features = ['log2', 'sqrt']
# Profundidade maxima da arvore
max_depth = [1, 6, 11, 16, 21, None]
# Numero minimo de amostras para separar um nó
min_samples_split = [2, 5, 10]
# Numero minimo de amostras para cada nó de folha
min_samples_leaf = [1, 2, 4]

parametros = {
               'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              }

random_search_rf_rus = RandomizedSearchCV(rf_rus, parametros, cv=5, scoring='recall', n_jobs=-1, random_state=42)
random_search_rf_rus.fit(X_treino_rus, y_treino_rus)

print("Melhores hiperparâmetros:", random_search_rf_rus.best_params_)
print("Melhor Recall na validação cruzada:", random_search_rf_rus.best_score_)

#### Teste

y_teste_proba_rf_rus = random_search_rf_rus.predict_proba(X_teste)[:, 1]
y_teste_pred_rf_rus = random_search_rf_rus.predict(X_teste)

acuracia_teste_rf_rus = accuracy_score(y_teste, y_teste_pred_rf_rus)
precisao_teste_rf_rus = precision_score(y_teste, y_teste_pred_rf_rus)
recall_teste_rf_rus = recall_score(y_teste, y_teste_pred_rf_rus)
f1_teste_rf_rus = f1_score(y_teste, y_teste_pred_rf_rus)
roc_auc_teste_rf_rus = roc_auc_score(y_teste, y_teste_proba_rf_rus)

print("Acurácia no teste:", acuracia_teste_rf_rus)
print("Precisão no teste:", precisao_teste_rf_rus)
print("Recall no teste:", recall_teste_rf_rus)
print("f1-Score no teste:", f1_teste_rf_rus)
print("ROC AUC no teste:", roc_auc_teste_rf_rus)

cm_teste_rf_rus = confusion_matrix(y_teste, y_teste_pred_rf_rus)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_rf_rus, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Regressão Logística - RandomOversampling

lr_rus = LogisticRegression(
                        random_state=42, # reproductibilidade
                        solver='liblinear', # bom para datasets pequenos
                        max_iter=1000 # permite alto número de iterações até encontrar a convergência
                        )

#### Treino

lr_rus.fit(X_treino_rus, y_treino_rus)

print('Convergência do modelo:', lr_rus.n_iter_)

coeficientes_rus = pd.DataFrame({'Variável': X.columns, 'Coeficiente': lr_rus.coef_[0]})
coeficientes_rus = coeficientes_rus.sort_values(by='Coeficiente', ascending=False)
print(coeficientes_rus)

#### Validação

y_val_proba_lr_rus = lr_rus.predict_proba(X_val)[:, 1]
y_val_pred_lr_rus = lr_rus.predict(X_val)

acuracia_val_lr_rus = accuracy_score(y_val, y_val_pred_lr_rus)
precisao_val_lr_rus = precision_score(y_val, y_val_pred_lr_rus)
recall_val_lr_rus = recall_score(y_val, y_val_pred_lr_rus)
f1_val_lr_rus = f1_score(y_val, y_val_pred_lr_rus)
roc_auc_val_lr_rus = roc_auc_score(y_val, y_val_proba_lr_rus)

print("Acurácia na validação:", acuracia_val_lr_rus)
print("Precisão na validação:", precisao_val_lr_rus)
print("Recall na validação:", recall_val_lr_rus)
print("f1-Score na validação:", f1_val_lr_rus)
print("ROC AUC na validação:", roc_auc_val_lr_rus)

cm_val_lr_rus = confusion_matrix(y_val, y_val_pred_lr_rus)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_lr_rus, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

parametros = {
              'C': [0.001, 0.01, 0.1, 1, 10, 100], # inverso da força de regularização
              'penalty': ['l1', 'l2'], # penalidade aplicada
              'class_weight': ['balanced', None], # pesos associados às classes, 'balanced' ajusta os pesos de modo inversamente proporcional a frequência das classes
        }

grid_search_lr_rus = GridSearchCV(
                                lr_rus,
                                parametros,
                                cv=5, # validação cruzada com 5 fols
                                scoring='recall' # otimizando para recall
                              )
grid_search_lr_rus.fit(X_treino_rus, y_treino_rus)

print("Melhores hiperparâmetros:", grid_search_lr_rus.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_lr_rus.best_score_)

#### Teste

y_teste_proba_lr_rus = grid_search_lr_rus.predict_proba(X_teste)[:, 1]
y_teste_pred_lr_rus = grid_search_lr_rus.predict(X_teste)

acuracia_teste_lr_rus = accuracy_score(y_teste, y_teste_pred_lr_rus)
precisao_teste_lr_rus = precision_score(y_teste, y_teste_pred_lr_rus)
recall_teste_lr_rus = recall_score(y_teste, y_teste_pred_lr_rus)
f1_teste_lr_rus = f1_score(y_teste, y_teste_pred_lr_rus)
roc_auc_teste_lr_rus = roc_auc_score(y_teste, y_teste_proba_lr_rus)

print("Acurácia no teste:", acuracia_teste_lr_rus)
print("Precisão no teste:", precisao_teste_lr_rus)
print("Recall no teste:", recall_teste_lr_rus)
print("f1-Score no teste:", f1_teste_lr_rus)
print("ROC AUC no teste:", roc_auc_teste_lr_rus)

cm_teste_lr_rus = confusion_matrix(y_teste, y_teste_pred_lr_rus)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_lr_rus, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

## SMOTE

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
smoted_X_treino, smoted_y_treino = sm.fit_resample(X_treino, y_treino)
X_treino.shape

smoted_X_treino.shape


### KNN - SMOTE

knn.fit(smoted_X_treino, smoted_y_treino)

#### Validação

y_val_proba_knn = knn.predict_proba(X_val)[:, 1]
y_val_pred_knn = knn.predict(X_val)

acuracia_val_knn = accuracy_score(y_val, y_val_pred_knn)
precisao_val_knn = precision_score(y_val, y_val_pred_knn)
recall_val_knn = recall_score(y_val, y_val_pred_knn)
f1_val_knn = f1_score(y_val, y_val_pred_knn)
roc_auc_val_knn = roc_auc_score(y_val, y_val_proba_knn)

print("Acurácia na validação:", acuracia_val_knn)
print("Precisão na validação:", precisao_val_knn)
print("Recall na validação:", recall_val_knn)
print("f1-Score na validação:", f1_val_knn)
print("ROC AUC na validação:", roc_auc_val_knn)

cm_val_knn = confusion_matrix(y_val, y_val_pred_knn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_knn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Hyperparameter tuning

n_neighbors = list(range(5, 83, 2)) # números impares de 5 ate 83
weights = ['uniform', 'distance']
algorithm= ['auto']
leaf_size= [30]
p= [2, 1]
metric=['minkowski', 'manhattan']
n_jobs= [-1]

parametros = {
              'n_neighbors': n_neighbors,
               'weights': weights,
               'algorithm': algorithm,
               'leaf_size': leaf_size,
               'p': p,
               'metric': metric,
               'n_jobs': n_jobs
        }

grid_search_knn = GridSearchCV(
                                knn,
                                parametros,
                                cv=5, # validação cruzada com 5 folds
                                scoring='recall' # otimizando para recall
                                )
grid_search_knn.fit(smoted_X_treino, smoted_y_treino)

print("Melhores hiperparâmetros:", grid_search_knn.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_knn.best_score_)

#### Teste

y_teste_proba_knn = grid_search_knn.predict_proba(X_teste)[:, 1]
y_teste_pred_knn = grid_search_knn.predict(X_teste)

acuracia_teste_knn = accuracy_score(y_teste, y_teste_pred_knn)
precisao_teste_knn = precision_score(y_teste, y_teste_pred_knn)
recall_teste_knn = recall_score(y_teste, y_teste_pred_knn)
f1_teste_knn = f1_score(y_teste, y_teste_pred_knn)
roc_auc_teste_knn = roc_auc_score(y_teste, y_teste_proba_knn)

print("Acurácia no teste:", acuracia_teste_knn)
print("Precisão no teste:", precisao_teste_knn)
print("Recall no teste:", recall_teste_knn)
print("f1-Score no teste:", f1_teste_knn)
print("ROC AUC no teste:", roc_auc_teste_knn)

cm_teste_knn = confusion_matrix(y_teste, y_teste_pred_knn)

plt.figure(figsize=(4, 4))
sns.heatmap(cm_teste_knn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Random Forest - SMOTE

rf.fit(smoted_X_treino, smoted_y_treino)

#### Validação

y_val_proba_rf = rf.predict_proba(X_val)[:, 1]
y_val_pred_rf = rf.predict(X_val)

acuracia_val_rf = accuracy_score(y_val, y_val_pred_rf)
precisao_val_rf = precision_score(y_val, y_val_pred_rf)
recall_val_rf = recall_score(y_val, y_val_pred_rf)
f1_val_rf = f1_score(y_val, y_val_pred_rf)
roc_auc_val_rf = roc_auc_score(y_val, y_val_proba_rf)

print("Acurácia na validação:", acuracia_val_rf)
print("Precisão na validação:", precisao_val_rf)
print("Recall na validação:", recall_val_rf)
print("f1-Score na validação:", f1_val_rf)
print("ROC AUC na validação:", roc_auc_val_rf)

cm_val_rf = confusion_matrix(y_val, y_val_pred_rf)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_rf, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Hyperparameter tuning

# Numero de arvores em floresta aleatoria
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)]
# Numero de features a serem consideradas em cada splita
max_features = ['log2', 'sqrt']
# Profundidade maxima da arvore
max_depth = [1, 6, 11, 16, 21, None]
# Numero minimo de amostras para separar um nó
min_samples_split = [2, 5, 10]
# Numero minimo de amostras para cada nó de folha
min_samples_leaf = [1, 2, 4]

parametros = {
               'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              }

random_search_rf = RandomizedSearchCV(rf, parametros, cv=5, scoring='recall', n_jobs=-1, random_state=42)
random_search_rf.fit(smoted_X_treino, smoted_y_treino)

print("Melhores hiperparâmetros:", random_search_rf.best_params_)
print("Melhor Recall na validação cruzada:", random_search_rf.best_score_)

#### Teste

y_teste_proba_rf = random_search_rf.predict_proba(X_teste)[:, 1]
y_teste_pred_rf = random_search_rf.predict(X_teste)

acuracia_teste_rf = accuracy_score(y_teste, y_teste_pred_rf)
precisao_teste_rf = precision_score(y_teste, y_teste_pred_rf)
recall_teste_rf = recall_score(y_teste, y_teste_pred_rf)
f1_teste_rf = f1_score(y_teste, y_teste_pred_rf)
roc_auc_teste_rf = roc_auc_score(y_teste, y_teste_proba_rf)

print("Acurácia no teste:", acuracia_teste_rf)
print("Precisão no teste:", precisao_teste_rf)
print("Recall no teste:", recall_teste_rf)
print("f1-Score no teste:", f1_teste_rf)
print("ROC AUC no teste:", roc_auc_teste_rf)

cm_teste_rf = confusion_matrix(y_teste, y_teste_pred_rf)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_rf, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Regressão Logística - SMOTE

#### Validação

lr.fit(smoted_X_treino, smoted_y_treino)

y_val_proba_lr = lr.predict_proba(X_val)[:, 1]
y_val_pred_lr = lr.predict(X_val)

acuracia_val_lr = accuracy_score(y_val, y_val_pred_lr)
precisao_val_lr = precision_score(y_val, y_val_pred_lr)
recall_val_lr = recall_score(y_val, y_val_pred_lr)
f1_val_lr = f1_score(y_val, y_val_pred_lr)
roc_auc_val_lr = roc_auc_score(y_val, y_val_proba_lr)

print("Acurácia na validação:", acuracia_val_lr)
print("Precisão na validação:", precisao_val_lr)
print("Recall na validação:", recall_val_lr)
print("f1-Score na validação:", f1_val_lr)
print("ROC AUC na validação:", roc_auc_val_lr)

cm_val_lr = confusion_matrix(y_val, y_val_pred_lr)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_lr, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Hyperparameter tuning

parametros = {
              'C': [0.001, 0.01, 0.1, 1, 10, 100], # inverso da força de regularização
              'penalty': ['l1', 'l2'], # penalidade aplicada
              'class_weight': ['balanced', None], # pesos associados às classes, 'balanced' ajusta os pesos de modo inversamente proporcional a frequência das classes
        }

grid_search_lr = GridSearchCV(
                                lr,
                                parametros,
                                cv=5, # validação cruzada com 5 fols
                                scoring='recall' # otimizando para recall
                              )
grid_search_lr.fit(smoted_X_treino, smoted_y_treino)

print("Melhores hiperparâmetros:", grid_search_lr.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_lr.best_score_)

#### Teste

y_teste_proba_lr = grid_search_lr.predict_proba(X_teste)[:, 1]
y_teste_pred_lr = grid_search_lr.predict(X_teste)

acuracia_teste_lr = accuracy_score(y_teste, y_teste_pred_lr)
precisao_teste_lr = precision_score(y_teste, y_teste_pred_lr)
recall_teste_lr = recall_score(y_teste, y_teste_pred_lr)
f1_teste_lr = f1_score(y_teste, y_teste_pred_lr)
roc_auc_teste_lr = roc_auc_score(y_teste, y_teste_proba_lr)

print("Acurácia no teste:", acuracia_teste_lr)
print("Precisão no teste:", precisao_teste_lr)
print("Recall no teste:", recall_teste_lr)
print("f1-Score no teste:", f1_teste_lr)
print("ROC AUC no teste:", roc_auc_teste_lr)

cm_teste_lr = confusion_matrix(y_teste, y_teste_pred_lr)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_lr, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

Aplicamos o smote, mas isso diminuiu o nosso recall

##SMOTEENN

 Esta técnica utiliza primeiramente o SMOTE para gerar novas amostras sintéticas e, após isso, usa o ENN (Edit Nearest Neighbors), que remove amostras quando seus vizinhos mais próximos são de outra classe.

from imblearn.combine import SMOTEENN
from imblearn.under_sampling import EditedNearestNeighbours

smenn = SMOTEENN(random_state=42, smote=SMOTE(random_state=42))
X_treino_smenn, y_treino_smenn = smenn.fit_resample(X_treino, y_treino)
print(X_treino.shape)
print(X_treino_smenn.shape)

print(y_treino_smenn.mean())

### KNN - SMOTEENN


#### Treino

knn_smenn = KNeighborsClassifier()

knn_smenn.fit(X_treino_smenn, y_treino_smenn)

#### Validação

y_val_proba_knn_smenn = knn_smenn.predict_proba(X_val)[:, 1]
y_val_pred_knn_smenn = knn_smenn.predict(X_val)

acuracia_val_knn_smenn = accuracy_score(y_val, y_val_pred_knn_smenn)
precisao_val_knn_smenn = precision_score(y_val, y_val_pred_knn_smenn)
recall_val_knn_smenn = recall_score(y_val, y_val_pred_knn_smenn)
f1_val_knn_smenn = f1_score(y_val, y_val_pred_knn_smenn)
roc_auc_val_knn_smenn = roc_auc_score(y_val, y_val_proba_knn_smenn)

print("Acurácia na validação:", acuracia_val_knn_smenn)
print("Precisão na validação:", precisao_val_knn_smenn)
print("Recall na validação:", recall_val_knn_smenn)
print("f1-Score na validação:", f1_val_knn_smenn)
print("ROC AUC na validação:", roc_auc_val_knn_smenn)

cm_val_knn_smenn = confusion_matrix(y_val, y_val_pred_knn_smenn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_knn_smenn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

n_neighbors = list(range(5, 83, 2)) # números impares de 5 ate 83
weights = ['uniform', 'distance']
algorithm= ['auto']
leaf_size= [30]
p= [2, 1]
metric=['minkowski', 'manhattan']
n_jobs= [-1]

parametros = {
              'n_neighbors': n_neighbors,
               'weights': weights,
               'algorithm': algorithm,
               'leaf_size': leaf_size,
               'p': p,
               'metric': metric,
               'n_jobs': n_jobs
        }

grid_search_knn_smenn = GridSearchCV(
                                knn_smenn,
                                parametros,
                                cv=5, # validação cruzada com 5 folds
                                scoring='recall' # otimizando para recall
                                )
grid_search_knn_smenn.fit(X_treino_smenn, y_treino_smenn)

print("Melhores hiperparâmetros:", grid_search_knn_smenn.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_knn_smenn.best_score_)

#### Teste

y_teste_smenn_proba_knn = grid_search_knn_smenn.predict_proba(X_teste)[:, 1]
y_teste_pred_knn_smenn = grid_search_knn_smenn.predict(X_teste)

acuracia_teste_knn_smenn = accuracy_score(y_teste, y_teste_pred_knn_smenn)
precisao_teste_knn_smenn = precision_score(y_teste, y_teste_pred_knn_smenn)
recall_teste_knn_smenn = recall_score(y_teste, y_teste_pred_knn_smenn)
f1_teste_knn_smenn = f1_score(y_teste, y_teste_pred_knn_smenn)
roc_auc_teste_knn_smenn = roc_auc_score(y_teste, y_teste_proba_knn)

print("Acurácia no teste:", acuracia_teste_knn_smenn)
print("Precisão no teste:", precisao_teste_knn_smenn)
print("Recall no teste:", recall_teste_knn_smenn)
print("f1-Score no teste:", f1_teste_knn_smenn)
print("ROC AUC no teste:", roc_auc_teste_knn_smenn)

cm_teste_knn_smenn = confusion_matrix(y_teste, y_teste_pred_knn_smenn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_knn_smenn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Random Forest - SMOTEENN

rf_smenn = RandomForestClassifier(random_state=42)

#### Treino

rf_smenn.fit(X_treino_smenn, y_treino_smenn)

#### Validação

y_val_proba_rf_smenn = rf_smenn.predict_proba(X_val)[:, 1]
y_val_pred_rf_smenn = rf_smenn.predict(X_val)

acuracia_val_rf_smenn = accuracy_score(y_val, y_val_pred_rf_smenn)
precisao_val_rf_smenn = precision_score(y_val, y_val_pred_rf_smenn)
recall_val_rf_smenn = recall_score(y_val, y_val_pred_rf_smenn)
f1_val_rf_smenn = f1_score(y_val, y_val_pred_rf_smenn)
roc_auc_val_rf_smenn = roc_auc_score(y_val, y_val_proba_rf_smenn)

print("Acurácia na validação:", acuracia_val_rf_smenn)
print("Precisão na validação:", precisao_val_rf_smenn)
print("Recall na validação:", recall_val_rf_smenn)
print("f1-Score na validação:", f1_val_rf_smenn)
print("ROC AUC na validação:", roc_auc_val_rf_smenn)

cm_val_rf_smenn = confusion_matrix(y_val, y_val_pred_rf_smenn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_rf_smenn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

# Numero de arvores em floresta aleatoria
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)]
# Numero de features a serem consideradas em cada splita
max_features = ['log2', 'sqrt']
# Profundidade maxima da arvore
max_depth = [1, 6, 11, 16, 21, None]
# Numero minimo de amostras para separar um nó
min_samples_split = [2, 5, 10]
# Numero minimo de amostras para cada nó de folha
min_samples_leaf = [1, 2, 4]

parametros = {
               'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              }

random_search_rf_smenn = RandomizedSearchCV(rf_smenn, parametros, cv=5, scoring='recall', n_jobs=-1, random_state=42)
random_search_rf_smenn.fit(X_treino_smenn, y_treino_smenn)

print("Melhores hiperparâmetros:", random_search_rf_smenn.best_params_)
print("Melhor Recall na validação cruzada:", random_search_rf_smenn.best_score_)

#### Teste

y_teste_proba_rf_smenn = random_search_rf_smenn.predict_proba(X_teste)[:, 1]
y_teste_pred_rf_smenn = random_search_rf_smenn.predict(X_teste)

acuracia_teste_rf_smenn = accuracy_score(y_teste, y_teste_pred_rf_smenn)
precisao_teste_rf_smenn = precision_score(y_teste, y_teste_pred_rf_smenn)
recall_teste_rf_smenn = recall_score(y_teste, y_teste_pred_rf_smenn)
f1_teste_rf_smenn = f1_score(y_teste, y_teste_pred_rf_smenn)
roc_auc_teste_rf_smenn = roc_auc_score(y_teste, y_teste_proba_rf_smenn)

print("Acurácia no teste:", acuracia_teste_rf_smenn)
print("Precisão no teste:", precisao_teste_rf_smenn)
print("Recall no teste:", recall_teste_rf_smenn)
print("f1-Score no teste:", f1_teste_rf_smenn)
print("ROC AUC no teste:", roc_auc_teste_rf_smenn)

cm_teste_rf_smenn = confusion_matrix(y_teste, y_teste_pred_rf_smenn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_rf_smenn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

### Regressão Logística - SMOTEENN

lr_smenn = LogisticRegression(
                        random_state=42, # reproductibilidade
                        solver='liblinear', # bom para datasets pequenos
                        max_iter=1000 # permite alto número de iterações até encontrar a convergência
                        )

#### Treino

lr_smenn.fit(X_treino_smenn, y_treino_smenn)

#### Validação

y_val_proba_lr_smenn = lr_smenn.predict_proba(X_val)[:, 1]
y_val_pred_lr_smenn = lr_smenn.predict(X_val)

acuracia_val_lr_smenn = accuracy_score(y_val, y_val_pred_lr_smenn)
precisao_val_lr_smenn = precision_score(y_val, y_val_pred_lr_smenn)
recall_val_lr_smenn = recall_score(y_val, y_val_pred_lr_smenn)
f1_val_lr_smenn = f1_score(y_val, y_val_pred_lr_smenn)
roc_auc_val_lr_smenn = roc_auc_score(y_val, y_val_proba_lr_smenn)

print("Acurácia na validação:", acuracia_val_lr_smenn)
print("Precisão na validação:", precisao_val_lr_smenn)
print("Recall na validação:", recall_val_lr_smenn)
print("f1-Score na validação:", f1_val_lr_smenn)
print("ROC AUC na validação:", roc_auc_val_lr_smenn)

cm_val_lr_smenn = confusion_matrix(y_val, y_val_pred_lr_smenn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_val_lr_smenn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

#### Tuning dos hiperparâmetros

parametros = {
              'C': [0.001, 0.01, 0.1, 1, 10, 100], # inverso da força de regularização
              'penalty': ['l1', 'l2'], # penalidade aplicada
              'class_weight': ['balanced', None], # pesos associados às classes, 'balanced' ajusta os pesos de modo inversamente proporcional a frequência das classes
        }

grid_search_lr_smenn = GridSearchCV(
                                lr_smenn,
                                parametros,
                                cv=5, # validação cruzada com 5 fols
                                scoring='recall' # otimizando para recall
                              )
grid_search_lr_smenn.fit(X_treino_smenn, y_treino_smenn)

print("Melhores hiperparâmetros:", grid_search_lr_smenn.best_params_)
print("Melhor Recall na validação cruzada:", grid_search_lr_smenn.best_score_)

#### Teste

y_teste_proba_lr_smenn = grid_search_lr_smenn.predict_proba(X_teste)[:, 1]
y_teste_pred_lr_smenn = grid_search_lr_smenn.predict(X_teste)

acuracia_teste_lr_smenn = accuracy_score(y_teste, y_teste_pred_lr_smenn)
precisao_teste_lr_smenn = precision_score(y_teste, y_teste_pred_lr_smenn)
recall_teste_lr_smenn = recall_score(y_teste, y_teste_pred_lr_smenn)
f1_teste_lr_smenn = f1_score(y_teste, y_teste_pred_lr_smenn)
roc_auc_teste_lr_smenn = roc_auc_score(y_teste, y_teste_proba_lr_smenn)

print("Acurácia no teste:", acuracia_teste_lr_smenn)
print("Precisão no teste:", precisao_teste_lr_smenn)
print("Recall no teste:", recall_teste_lr_smenn)
print("f1-Score no teste:", f1_teste_lr_smenn)
print("ROC AUC no teste:", roc_auc_teste_lr_smenn)

cm_teste_lr_smenn = confusion_matrix(y_teste, y_teste_pred_lr_smenn)

plt.figure(figsize=(6, 5))
sns.heatmap(cm_teste_lr_smenn, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Não evadiu", "Evadiu"],
            yticklabels=["Não evadiu", "Evadiu"])

plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()
